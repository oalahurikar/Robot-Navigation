# üß† 2D Point-Robot Navigator - Complete Project Outline
=======================================================

## Problem Statement
Learn a policy œÄ that maps minimal robot sensing to actions that reach a goal without collisions in cluttered 2D worlds.

## Complete Pipeline Overview
```
1. Generate 10x10 Environment
   ‚Üì
2. Use A* to find optimal path
   ‚Üì
3. Extract 3x3 perceptions along path
   ‚Üì
4. Convert movements to 4 actions
   ‚Üì
5. Create training examples
   ‚Üì
6. Train Neural Network
   ‚Üì
7. Evaluate Performance
```

## Technical Dependencies
- **Python**: 3.8+ (recommended 3.9+)
- **PyTorch**: 1.12+ (neural network implementation)
- **NumPy**: 1.21+ (numerical computations)
- **Matplotlib**: 3.5+ (visualization)
- **Scikit-learn**: 1.0+ (data splitting and metrics)
- **PyYAML**: 6.0+ (configuration management)

## Phase 1: Data Generation & Training Data
### Core Functionality

Step 1: Environment Generation (World)
Objective: Create diverse, random 2D grid worlds for training
Minimal Requirements:
- Grid Size: 10x10 (start simple, scale up later)
- Obstacle Density: 15-25% coverage (adjustable parameter)
- Start/Goal Placement: Random valid positions
- Validation: Ensure path exists between start and goal

üó∫Ô∏è ENVIRONMENT:
"R = Robot, G = Goal, X = Obstacle, . = Empty"
R . X . . 
. X X . . 
. . . . . 
X . . X . 
. . . . G 

Key Design Decisions:
```
1. Obstacle Patterns:
   - Random scattered obstacles

2. Difficulty Levels:
   - Easy: Sparse obstacles, direct paths
   - Medium: Moderate obstacles, some detours needed
   - Hard: Dense obstacles, complex navigation required

3. Environment Types:
   - Maze-like structures
   - Open Spaces with Scattered Obstacles
   ```
Example A - Simple Maze:
R X X X X X X X X X
. . . . . . . . . X
X X X X X X X X . X
X . . . . . . . . X
X . X X X X X X X X
X . X . . . . . . .
X . X . X X X X X .
X . . . X . . . . .
X X X X X . X X X G
. . . . . . . . . .

Example B - Complex Maze:
R . X X X X X X X X
X . . . . . . . . X
X X X . X X X X . X
. . . . X . . . . X
. X X X X . X X X X
. . . . . . X . . .
X X X . X X X . X .
X . . . X . . . X .
X . X X X . X X X G
X . . . . . . . . .

Example A - Sparse (15% density):
R . . . . . . . . G
. . X . . . . X . .
. . . . . . . . . .
. X . . . . . . X .
. . . . . . . . . .
. . . . . . . . . .
. . X . . . . . . .
. . . . . . . . . .
. X . . . . . . . .
. . . . . . . . . .

Example B - Dense (25% density):
R . X . . . . X . G
. X . X . . X . X .
. . . . . . . . . .
. . X . . X . . . .
. . . . . . . . . .
X . . . . . . . X .
. . X . . . . . . .
. . . . X . . . . .
. X . . . . . X . .
. . . . . . . . . .

---

Step 2: Expert Pathfinding (A* Algorithm)
Objective: Generate optimal paths for each environment
A* Implementation Strategy:
1. Use Manhattan distance as heuristic
2. Handle 4-connected movement (up, down, left, right)
3. Return complete path from start to goal
4. Validate path exists before proceeding
Path Quality Assurance:
- Check path exists (no impossible scenarios)
- Verify path is optimal (shortest possible)
- Handle edge cases (start = goal, blocked paths)

---
Step 3: State Extraction   
Objective: Extract robot's perception at each step
- Robot sees 3x3 grid around its position
- Input size: 9 values (flattened 3x3)
- Biological inspiration: Limited peripheral vision

---

## Phase 2: Neural Network Architecture & Training

### Step 4: Neural Network Design
Objective: Create biologically-inspired neural network for navigation

**Architecture:**
```
Input Layer: 9 neurons (3√ó3 perception)
Hidden Layer 1: 64 neurons + ReLU + Dropout(0.2)
Hidden Layer 2: 32 neurons + ReLU + Dropout(0.2)
Output Layer: 4 neurons + Softmax
```

**Key Design Decisions:**
- **ReLU Activation**: Mimics spiking neurons, prevents vanishing gradients
- **Softmax + Cross-Entropy**: Multi-class classification with competition
- **Dropout(0.2)**: Regularization to prevent overfitting
- **3-Way Data Split**: 80% train, 10% validation, 10% test

**Biological Inspiration:**
- **Local Perception**: 3√ó3 grid mimics animal peripheral vision
- **Sparse Coding**: ReLU creates sparse representations like brain
- **Neural Competition**: Softmax mimics winner-take-all competition
- **Robustness**: Dropout simulates neural noise and failures
- **Learning**: Backpropagation mimics synaptic plasticity

### Step 5: Training Pipeline
Objective: Train neural network with optimal hyperparameters

**Training Strategy:**
- **Optimizer**: Mini-batch gradient descent
- **Learning Rate**: 0.001 with early stopping
- **Batch Size**: 32 samples per batch
- **Epochs**: 100 maximum with early stopping (patience=15)
- **Regularization**: Dropout(0.2) for generalization

**Configuration Management:**
- **YAML Config**: All hyperparameters in `configs/nn_config.yaml`
- **Modular Design**: Easy to modify and experiment
- **Reproducible**: Fixed random seeds for consistent results

### Step 6: Model Evaluation
Objective: Assess neural network performance

**Metrics:**
- **Accuracy**: Classification accuracy on test set
- **Loss**: Cross-entropy loss for training monitoring
- **Confusion Matrix**: Per-action performance analysis
- **Training Curves**: Loss and accuracy over epochs

**Expected Performance:**
- **Training Accuracy**: 85-95% (learning training patterns)
- **Validation Accuracy**: 80-90% (generalization ability)
- **Test Accuracy**: 80-90% (true performance estimate)

---
